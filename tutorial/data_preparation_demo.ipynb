{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to run this notebook (command-line)?**\n",
    "1. Install the `lib_invent_data` environment:\n",
    "`conda env -f environment.yml`\n",
    "2. Activate the environment:\n",
    "`conda activate lib_invent_data`\n",
    "3. Execute `jupyter`:\n",
    "`jupyter notebook`\n",
    "4. Copy the link to a browser\n",
    "\n",
    "\n",
    "# `Lib-INVENT Datasets`: Data preparation demo\n",
    "This demo illustrates how to compute the distribution of the chemical properties and based on the properties filter data from ChEMBL or other sources to only include drug-like molecules. \n",
    "\n",
    "To proceed, please update the following code block such that it reflects your system's installation and execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "> **There are a number of reasons to pre-process the data used for training a generative model.**\n",
    "1. Removal of invalid or duplicated entries.\n",
    "2. Removal of unusual compounds that are clearly not drug-like (too big, reactive groups and etc.). There is normally no point training model on such examples since that bias will reflected by the generative model. \n",
    "3. Removal of rare tokens. There are rare compounds that can be seen as outliers. They in turn might contain rare tokens. Excluding them frees a slot in the vocabulary and makes it smaller. Smaller vocabulary means faster training and less memory. As a result removing compounds that introduce rare tokens to the vocabulary speeds up the generative model.\n",
    "\n",
    "### Introduction\n",
    "This configuration can be used for preparing data to only include drug-like molecules or calculate stats for sliced datasets. This Demo mainly focuses on preparing and filtering data.\n",
    "> **The rules used for filtering data:**\n",
    "- 2 <= num heavy atoms <= 70   \n",
    "- allowed elements: [6, 7, 8, 9, 16, 17, 35]  \n",
    "- remove salts, neutralize charges, sanitize\n",
    "- remove side chains with 5 or more carbon atoms\n",
    "- 0<num_rings <= 10\n",
    "- num_atoms >= 6\n",
    "- mol_weights <= 760\n",
    "- num_aromatic_rings <= 8\n",
    "- heteroatom_ratio > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "# --------- change these path variables as required\n",
    "data_path = \"<path/to/chembl/smiles>\" # for the training of Lib-INVENT, we used ChEMBL 27 and converted to SMILES using RDKit.\n",
    "output_dir = \"<path/to/your/output_directory/>\"\n",
    "\n",
    "# --------- do not change\n",
    "# get the notebook's root path\n",
    "try: ipynb_path\n",
    "except NameError: ipynb_path = os.getcwd()  \n",
    "\n",
    "# if required, generate a folder to store the results\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the configuration\n",
    "`Lib-INVENT datasets` has an entry point that loads a specified `JSON` file on startup. `JSON` is a low-level data format that allows to specify a fairly large number of parameters in a cascading fashion very quickly. The parameters are structured into *blocks* which can in turn contain blocks or simple values, such as *True* or *False*, strings and numbers. In this tutorial, we will go through the different blocks step-by-step, explaining their purpose and potential values for given parameters. Note, that while we will write out the configuration as a `JSON` file in the end, in `python` we handle the same information as a simple `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary\n",
    "configuration = {\n",
    "    \"run_type\": \"stats_extraction\"                                          \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Standardization_config` contains rules to standardise the molecules using functions in reinvent chemistry. This standardization includes:\n",
    "- 2 <= num heavy atoms <= 70\n",
    "- allowed elements: [6, 7, 8, 9, 16, 17, 35]\n",
    "- remove salts, neutralise charges, sanitize\n",
    "- remove side chains with 5 or more carbon atoms\n",
    "\n",
    "`filter` includes filtering rules that wanted to be applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration[\"parameters\"] = {\n",
    "    \"data_path\": data_path,                 # location to store input data      \n",
    "    \"output_path\": output_dir,              # location to store the results\n",
    "    \"properties\": ['mol_wts',               # properties of interest, avaliable properties:\n",
    "                   'num_rings',              # 'mol_wts', 'num_rings', 'num_aromatic_rings', 'num_atoms',   \n",
    "                   'num_aromatic_rings',     #'hbond_donors', 'hbond_acceptors'\n",
    "                   'num_atoms',\n",
    "                   'hbond_donors',\n",
    "                   'hbond_acceptors'],\n",
    "    \"token_distribution\": True,             # calculate the counts of individual tokens\n",
    "    \"columns\": [\"original\"],                # other options for sliced datasets: \"scaffolds\",\"decorations\"  \n",
    "    \"mode\": \"orig_data\",                    # other option:\"sliced_data\"\n",
    "    \"plotting\": True,                       # plot the distribution of chemical properties\n",
    "    \"standardisation_config\":{\"neutralise_charges\":  {\"reactions\": None}},            # standardize molecules\n",
    "    \"save_standardised\": True,              # save the standardised dataframe\n",
    "    \"filter\": {                             # filter contains properties conditions that want to be filtered,\n",
    "        \"num_rings\": [\"max\", 10],            #\"min\": \">=\",\"max\": \"<=\"\n",
    "        \"num_rings\": [\"min\", 1],\n",
    "        \"num_atoms\": [\"min\",6],\n",
    "        \"mol_wts\": [\"max\", 760],\n",
    "        \"num_aromatic_rings\": [\"max\", 8]\n",
    "    },                                    \n",
    "                                                          \n",
    "    \"save_cut_precomputed\": True,          # save the filtered dataframe\n",
    "    \"token_atom_ratio\": True,              # if the ratio is too high then the molecule is too complicated for the \n",
    "                                             # model to learn\n",
    "    \"count_decorations\": False             \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the configuration file to the disc\n",
    "configuration_JSON_path = os.path.join(output_dir, \"data_preparation_example.json\")\n",
    "with open(configuration_JSON_path, 'w') as f:\n",
    "    json.dump(configuration, f, indent=4, sort_keys= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Execute in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_err_stream --no-stderr \n",
    "\n",
    "# execute\n",
    "%cd </path/to/Lib-DESIGN-datasets/project/directory/>\n",
    "!spark-submit --driver-memory=32g --conf spark.driver.maxResultSize=16g input.py <configuration_JSON_path>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output to a file, just to have it for documentation\n",
    "with open(os.path.join(output_dir, \"run.err\"), 'w') as file:\n",
    "    file.write(captured_err_stream.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute in command line\n",
    "```\n",
    "# activate envionment\n",
    "conda activate lib_invent_data\n",
    "\n",
    "# go to the root folder of input.py \n",
    "cd </path/to/Lib-INVENT-datasets/directory>\n",
    "\n",
    "# execute in command line\n",
    "spark-submit --driver-memory=32g --conf spark.driver.maxResultSize=16g input.py </path/to/configuratoin.json>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
