{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# `Lib-INVENT Dataset`: Reaction Based Slicing Demo\n",
    "The purpose of this notebook is to illustrate how to generate a configuration for slicing compounds in a dataset based on reactions.\n",
    "\n",
    "Inputs:\n",
    "- A `dataset` with compound SMILES (typically already filtered to exclude non-drug like compounds). For the training of Lib-INVENT, the filtered ChEMBL 27 smiles were used. This dataset can be found in the Lib-INVENT project repository.\n",
    "- `reaction.smirks` file which contains reactions that will be used to slice the compounds.\n",
    "\n",
    "The `output SMILES file` will contain three columns including `scaffolds`, `decorations` and the `original compounds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "# --------- change these path variables as required\n",
    "lib_invent_dataset_project_dir = \"<path/to/your/project/>\"\n",
    "input_dataset_path = \"</path/to/input_smiles_file>\" \n",
    "                                                                                                  \n",
    "reactions_file_path= os.path.join(lib_invent_dataset_project_dir, \"tutorial/data/reaction.smirks\")\n",
    "output_dir = \"</path/to/output_directory/>\"\n",
    "\n",
    "# --------- do not change\n",
    "# get the notebook's root path\n",
    "try: ipynb_path\n",
    "except NameError: ipynb_path = os.getcwd()\n",
    "\n",
    "# if required, generate a folder to store the results\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the configuration\n",
    "`Lib-INVENT dataset` has an entry point that loads a specified `JSON` file on startup. Similar to the first tutorial, the remainder of this notebook demonstrates the process of the assembly of the dictionary which can then be converted to the desired `JSON` file saven on a local machine.\n",
    "\n",
    "In contrast to the `stats_extraction` running mode, `reaction_based_slicing` is a relatively straightforward operation which requires fewer arguments. As mentioned above, the essential inputs needed is a dataset of compounds to be sliced, provided in a text file with one SMILES per line, and a file of reaction SMIRKS, which is provided along with these tutorials. Finally, technicalities of the slicing are specified: this includes the maximum number of cuts per compound and conditions about the form of the resulting fragments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Condition Configuration\n",
    "A `condition configuration` will be assembled first. This configuration will be used as an input in the reaction based slicing configuration. It specifies particular conditions the scaffolds and decorations have to satisfy in order to be included in the sliced dataset. \n",
    "\n",
    "No conditions on the decorations were imposed when preprocessing data for the Lib-INVENT publication. The scaffolds are required to contain at least one ring. This assumption is motivated by the fact that in typical library design scenarios, the base scaffold is an aromatic compound; simple scaffolds without rings do not carry significant chemical properties and are therefore not typically useful for lead optimisation applications as described in the Lib-INVENT publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_configuration={\n",
    "    \"scaffold\": [{\n",
    "        \"name\":\"ring_count\",\n",
    "        \"min\": 1\n",
    "    }],\n",
    "    \"decoration\": [\n",
    "    ]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the configuration file to the disc\n",
    "condition_configuration_JSON_path = os.path.join(output_dir, \"filter_conditions.json\")\n",
    "with open(condition_configuration_JSON_path, 'w') as f:\n",
    "    json.dump(condition_configuration, f, indent=4, sort_keys= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Reaction Based Slicing Configuration\n",
    "\n",
    "The JSON configuration passed to the Lib-INVENT Dataset input contains two blocks: `run_type` and `parameters`. In these, the necessary arguments are passed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the dictionary\n",
    "configuration = {\n",
    "    \"run_type\": \"reaction_based_slicing\"                                          \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration[\"parameters\"] = {\n",
    "    \"input_file\": input_dataset,\n",
    "    \"output_path\": os.path.join(output_dir, \"sliced\"),\n",
    "    \"output_smiles_file\": os.path.join(output_dir, \"reaction_smiles\"),\n",
    "    \"conditions_file\": condition_configuration_JSON_path,\n",
    "    \"reactions_file\": reactions_file_path,\n",
    "    \"max_cuts\": 4,                           # the maximum number of cuts to perform on each molecule.\n",
    "    \"number_of_partitions\": 1000,            # relevant for PySpark. Do not change.\n",
    "    \"validate_randomization\": True           # check that randomised molecules correspond to the originals.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the configuration file to the disc\n",
    "configuration_JSON_path = os.path.join(output_dir, \"reaction_slicing_config.json\")\n",
    "with open(configuration_JSON_path, 'w') as f:\n",
    "    json.dump(configuration, f, indent=4, sort_keys= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Execute in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_err_stream --no-stderr \n",
    "\n",
    "# execute\n",
    "%cd {lib_invent_dataset_project_dir}\n",
    "!spark-submit --driver-memory=80g --conf spark.driver.maxResultSize=32g input.py {configuration_JSON_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output to a file, just to have it for documentation\n",
    "with open(os.path.join(output_dir, \"run.err\"), 'w') as file:\n",
    "    file.write(captured_err_stream.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute in command line\n",
    "```\n",
    "# activate environment\n",
    "conda activate lib_invent_data\n",
    "\n",
    "# go to the root folder of input.py \n",
    "cd </path/to/Lib-INVENT-dataset/directory>\n",
    "\n",
    "# execute in command line\n",
    "spark-submit --driver-memory=32g --conf spark.driver.maxResultSize=16g input.py </path/to/configuration.json>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
